# 807补充（十）（泛函极值篇）

## 一.泛函

具有某种共同性质的函数构成的集合称为类函数或函数类,记作 $F$ 。例如, 在罗杰斯特回归中, 所有的模型都由参数$\boldsymbol W$控制，而参数$\boldsymbol W$就是函数集合所具有的共同性质（虽然每个具体模型的参数不一样）。

常见的类函数有:在开区间 $\left(x_0, x_1\right)$ 内连续的函数集, 称为在区间 $\left(x_0, x_1\right)$ 上的连续函数类, 记为 $C\left(x_0, x_1\right)$ 。在闭区间 $\left[x_0, x_1\right]$ 上连续的函数集, 称为在区间 $\left[x_0, x_1\right]$ 上的连续函数类, 记为 $C\left[x_0, x_1\right]$,其中函数在区间的左端点右连续, 在区间的右端点左连续。这时它在区间端点的连续称为单边连续。

在开区间 $\left(x_0, x_1\right)$ 内, $n$ 阶导数连续的函数集, 称为在区间 $\left(x_0, x_1\right)$ 上 $n$ 阶导数连续的函数类,记为  $C^n\left(x_0, x_1\right)$, 并约定 $C^0\left(x_0, x_1\right)=C\left(x_0, x_1\right)$, 即类函数的零阶导数就是该类函数本身。

设 $F=\{y(x)\}$ 是给定的某一类函数, $\Bbb R$ 为实数集合。如果对于类函数 $F$ 中的每一个函数 $y(x)$, 在 $\Bbb R$ 中变量 $J$ 都有一个确定的数值按照一定的规律与之对应, 则 $J$ 称为 (类函数 $F$中) 函数 $y(x)$ 的泛函, 记为 $J=J[y(x)] 、 J=J[y(\cdot)]$ 或 $J=J[y]$ 。函数 $y(x)$ 称为**泛函** $J$的**宗量**, 有时也称为**泛函变量**、**宗量函数**或**变函数**。**类函数 $F$ 称为泛函 $J$ 的定义域。加在宗量函数上的条件称为容许条件。**属于定义域的宗量函数称为可取函数或容许函数。换句话说, **泛函是以类函数为定义域的实值函数。泛函是函数概念的推广。为了与普通函数相区别,泛函所依赖的函数用方括号括起来。**

在机器学习领域，广泛使用的泛函是连续变量$x$的熵$H(x)$.

| 函数          | 泛函               | 函数                         | 泛函                          |
| :------------ | :----------------- | :--------------------------- | :---------------------------- |
| 函数 $f(x)$   | 泛函 $J=[y(x)]$    | 自变量 $x$ 的增量 $\Delta x$ | 函数 $y(x)$ 的变分 $\delta J$ |
| 变量 $y=f(x)$ | 变量 $J=J[y(x)]$   | 函数的微分 $\mathrm{d} y$    | 泛函的变分 $\delta J$         |
| 自变量 $x$    | 自变量 函数 $y(x)$ |                              |                               |



同普通函数一样，泛函也有极值，求解泛函极值的问题称为变分问题。

例 设 $0 \leqslant x_0<x_1$, 试证泛函 $J[y(x)]=\int_{x_0}^{x_1}\left(x^2+y^2\right) \mathrm{d} x$ 在曲线 $y(x) \equiv 0$ 上取得全局极小值。
		证： 因 $0 \leqslant x_0<x_1$, 故对于任意一个在 $\left[x_0, x_1\right]$ 上连续的函数 $y(x)$, 有
$$
\Delta J=J[y(x)]-J[0]=\int_{x_0}^{x_1}\left(x^2+y^2\right) \mathrm{d} x-\int_{x_0}^{x_1} x^2 \mathrm{~d} x=\int_{x_0}^{x_1} y^2 \mathrm{~d} x \geqslant 0
$$

等式仅当 $y(x) \equiv 0$ 时成立。

## 二.固定端点变分问题

考虑函数$y(x)$已固定端点，即在容许集中任一函数都使得$y(x_0)=a,y_(x_1)=b$恒成立。

对于任意定值 $x \in\left[x_0, x_1\right]$, 可取函数 $y(x)$ 与另一可取函数 $y_0(x)$ 之差 $y(x)-y_0(x)$ 称为函数 $y(x)$ 在 $y_0(x)$ 处的变分或函数的变分, 记作 $\delta y, \delta$ 称为变分记号、变分符号或变分算子, 这时有
$$
\delta y=y(x)-y_0(x)=\epsilon \eta(x)
$$

式中, $\epsilon$ 为拉格朗日引进的一个小参数,但它不是 $x$ 的函数; $\eta(x)$ 为 $x$ 的任意函数。**由于可取函数都通过区间的固定端点, 即它们在区间的端点的值都相等**, 故在区间的端点, 任意函数 $\eta(x)$ 都满足
$$
\eta\left(x_0\right)=\eta\left(x_1\right)=0
$$

也就是
$$
\delta y\left(x_0\right)=\delta y\left(x_1\right)=0
$$
在传统的微积分中的一个常见问题是找到一个$x$值使得$y(x)$取得极值，类似的，变分法中我们寻找一个函数$y(x)$来求得泛函的极值。对$y(x)$施加微扰$\epsilon$考虑函数$y(x)$的泰勒数展开。在极值点处应有$\frac{dy}{dx}=0$成立.
$$
y(x+\epsilon)=y(x)+\frac{\text dy}{\text dx}\epsilon+O(\epsilon)\tag{1.1}
$$
类似地，我们可以得到泛函的导数定义。考虑最简泛函$J[y]=\int G(y\prime(x),y(x),x)\text dx$,当我们对函数$y(x)$施加一个微小的扰动$\epsilon\eta(x)$时，我们考虑泛函$y(x)$的变化,(这里用到了变分与求导的换序)
$$
\begin{aligned}
J[y(x)+\epsilon\eta(x)]&=\int G(y\prime+\epsilon\eta\prime,y+\epsilon\eta,x)\text dx\\
&=\int[G(y\prime,y,x)+\epsilon\frac{\text dG}{\text dy\prime}\eta\prime+\epsilon\frac{\text dG}{\text dy}\eta+O(\epsilon)]\text dx\\
&=J[y(x)]+\epsilon\int[\frac{\text dG}{\text dy}\eta(x)+\frac{\text dG}{\text dy\prime}\eta\prime(x)]\text dx+O(\epsilon)
\end{aligned}
\tag{1.2}
$$
或者可以写成
$$
J[y(x)+\epsilon\eta(x)]=J[y(x)]+\epsilon\int\frac{\text dJ}{\text dy(x)}\eta(x)\text dx+O(\epsilon)\tag{1.3}
$$


类比公式（1.1）在极值处应有$\int\frac{\text dF}{\text dy(x)}\eta(x)\text dx=0$对于任意$\eta(x)$成立,因此我们必须令泛函的导数为$0$。

利用分布积分可得
$$
\begin{aligned}
\int[\frac{\text dG}{\text dy\prime}\eta\prime(x)]\text dx&=\int\frac{\text dG}{\text dy\prime}\text d\eta(x)\\
&=\eta(x)\frac{\text dG}{\text dy\prime}\mid_x-\int\eta(x)\text d\frac{\text dG}{\text dy\prime}\\
\end{aligned}
\tag{1.4}
$$
又因固定边界，则$\eta(x)$在边界上都为0，由此可得
$$
\int[\frac{\text dG}{\text dy\prime}\eta\prime(x)]\text dx=-\int\eta(x)\text d\frac{\text dG}{\text dy\prime}
\tag{1.5}
$$
最终可以得到
$$
J[y(x)+\epsilon\eta(x)]=J[y(x)]+\epsilon\int[\frac{\text dG}{\text dy}-\frac{\text d}{\text dx}(\frac{\text dG}{\text dy\prime})]\eta(x)\text dx+O(\epsilon)
\tag{1.6}
$$
与式1.3对比可推出：
$$
\frac{\text dJ}{\text dy(x)}=\frac{\text dG}{\text dy}-\frac{\text d}{\text dx}(\frac{\text dG}{\text dy\prime})
\tag{1.7}
$$
$F[x]$在极值点处满足,即**欧拉-拉格朗日方程**：
$$
\frac{\text dJ}{\text dy(x)}=\frac{\text dG}{\text dy}-\frac{\text d}{\text dx}(\frac{\text dG}{\text dy\prime})=0
\tag{1.8}
$$


如果欧拉方程中的 $G_{y^{\prime} y^{\prime}} \neq 0$, 则式 1.8是一个二阶常微分方程, 所讨论的变分问题归结为解如下的微分方程边值问题
$$
\left\{\begin{array}{l}
\frac{\text dG}{\text dy}-\frac{\text d}{\text dx}(\frac{\text dG}{\text dy\prime})=0 \\
y\left(x_0\right)=a, y\left(x_1\right)=b
\end{array}\right.
$$
其通解含有两个任意常数
$$
y=y\left(x, c_1, c_2\right)
$$

它的图形称为欧拉方程的积分曲线, 也称为泛函极值曲线族或极值曲线簇, 其中两个任意常数可由边界条件来确定。

例： 求泛函 $J[y]=\int_0^{\frac{\pi}{2}}\left(y^{\prime 2}-y^2\right) \mathrm{d} x$ 的极值曲线, 边界条件为 $y(0)=0, y\left(\frac{\pi}{2}\right)=1$ 。

解 令 $G=y^{\prime 2}-y^2$, 泛函的欧拉方程为
$$
G_y-\frac{\mathrm{d}}{\mathrm{d} x} G_{y^{\prime}}=-2 y-2 y^{\prime \prime}=0
$$

即 $y^{\prime \prime}+y=0$, 其通解是 $y=c_1 \cos x+c_2 \sin x$ 。利用边界条件可得 $c_1=0, c_2=1$ 。故极值曲线是 $y=\sin x$ 。

欧拉方程得出的解仅仅是极值的必要边界条件，想要知道是否是极值，以及是极大还是极小依赖与泛函的二阶变分。

>定理 如果曲线 $y=y(x)\left(x_0 \leqslant x \leqslant x_1\right)$ 是泛函 满足边界条件的极值曲线, 且满足雅可比条件, 在极值曲线 $y=y(x)$ 上, 勒让德条件成立, 即 $G_{y^{\prime} y^{\prime}}\left(x, y, y^{\prime}\right)$ 不改变符号; 则满足周定边界条件的泛函  在极值曲线 $y=y(x)$ 上取得弱极值。当 $G_{y y^{\prime}}>0$ 时,取得弱极小值; 当 $G_{y y^{\prime}}<0$ 时取得弱极大值。

一般认为在现实中的函数都具有良好的性质，雅可比条件自动满足。

## 三.可动边界变分问题

设泛函
$$
J[y(x)]=\int_{x_0}^{x_1} G\left(x, y, y^{\prime}\right) \mathrm{d} x\tag{1.9}
$$

其可取曲线 $y=y(x) \in C^2$ 类函数, 且两个端点 $A\left(x_0\right.$, $\left.y_0\right) 、 B\left(x_1, y_1\right)$ 分别在两个给定的 $C^2$ 类函数 $y=\varphi(x)$与 $y=\psi(x)$ 上移动 。此时, 上式称为可动边界的最简泛函或待定边界的最简泛函。

若函数 $y=y(x)$ 能在可动边界的容许函数类中使泛函 1.9 取得极值, 那么必能在固定边界的容许函数类中使泛函取得极值, 这是因为可动边界泛函的容许曲线类的范围扩大了, 当然包含了固定边界泛函的容许曲线, 而在固定边界情况下使泛函取得极值的函数必须满足欧拉方程, 所以函数 $y=y(x)$ 在可动边界情况下也应当满足欧拉方程
两端点可移动曲线
$$
G_y-\frac{\mathrm{d}}{\mathrm{d} x} G_{y^{\prime}}=0
$$

欧拉方程的解含有两个任意常数, 它的一般形式为
$$
y=y \quad\left(x, c_1, c_2\right)
$$
在端点固定的情况下, 这两个常数可由边界条件 $y_0=y\left(x_0\right)$ 和 $y_1=y\left(x_1\right)$ 确定。而在可动边界条件下,它们都是 $x_0$ 和 $x_1$ 的函数,且 $x_0$ 和 (或) $x_1$ 也是待定的。确定它们的条件就是泛函取得极值的必要条件 $\delta J=0$ 。

> 自然边界条件(冯诺伊曼边界条件)
> $$
> \left.G_{y^{\prime}}\right|_{x=x_1}=0
> $$
> 本质边界条件(狄利克雷边界条件)
> $$
> \left.\left(G-(\varphi^{\prime}-y^{\prime}) G_{y^{\prime}}\right)\right|_{x=x_1}=0
> $$

若泛函一边在固定直线$x=x_0$上待定，则其满足自由边界条件。

若泛函一边在曲线$y=\varphi(x)$上待定，则其满足本质边界条件。

例 求泛函 $J[y]=\int_{x_0}^{x_1}\left[p(x) y^{\prime 2}+q(x) y^2+2 f(x) y\right] \mathrm{d} x$ 极值问题的自然边界条件, 其中 $x_0$ 和 $x_1$ 均为自由边界, $p(x) 、 q(x)$ 和 $f(x)$ 均为已知函数, 且 $p(x) \neq 0$ 。
解 因 $x_0$ 和 $x_1$ 均为自由边界, 根据定理 4.1.1, 故自然边界条件为
$$
\left.G_{y^{\prime}}\right|_{x-x_0}=\left.2 p(x) y^{\prime}\right|_{x=x_0}=0,\left.\quad G_{y^{\prime}}\right|_{x=x_1}=\left.2 p(x) y^{\prime}\right|_{x-x_1}=0
$$

由于 $p(x) \neq 0$, 故自然边界条件可化为
$$
\left.y^{\prime}\right|_{x=x_0}=y^{\prime}\left(x_0\right)=0,\left.\quad y^{\prime}\right|_{x=x_1}=y^{\prime}\left(x_1\right)=0
$$

## 四.例

- 考虑最大熵分布

$$
\begin{aligned}
\text{maxmize}\qquad &H(\boldsymbol x)=-\int p(\boldsymbol x)\ln p(\boldsymbol x)\text d\boldsymbol x\\
\text{s.t}\qquad &\int p(\boldsymbol x)\text{d}\boldsymbol x=\boldsymbol 1\\
&\int\boldsymbol x p(\boldsymbol x)\text d\boldsymbol x=\boldsymbol  \mu\\
&\int(\boldsymbol x-\boldsymbol \mu)(\boldsymbol x-\boldsymbol \mu)^{\text T}p(\boldsymbol x)\text d\boldsymbol x=\boldsymbol \Sigma
\end{aligned}
$$

引入拉格朗日乘子
$$
\begin{aligned}
\widetilde{\mathrm{H}}[p]= & -\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x}+\lambda\left(\int p(\mathbf{x}) \mathrm{d} \mathbf{x}-1\right) \\
& +\mathbf{m}^{\mathrm{T}}\left(\int p(\mathbf{x}) \mathbf{x} \mathrm{d} \mathbf{x}-\boldsymbol{\mu}\right) \\
& +\operatorname{Tr}\left\{\mathbf{L}\left(\int p(\mathbf{x})(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathrm{d} \mathbf{x}-\mathbf{\Sigma}\right)\right\} .
\end{aligned}
$$

求导得
$$
0=-1-\ln p(\mathbf{x})+\lambda+\mathbf{m}^{\mathrm{T}} \mathbf{x}+\operatorname{Tr}\left\{\mathbf{L}(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}\right\} .
$$

$$
p(\mathbf{x})=\exp \left\{\lambda-1+\mathbf{m}^{\mathrm{T}} \mathbf{x}+(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{L}(\mathbf{x}-\boldsymbol{\mu})\right\} .
$$

进行配方得
$$
\lambda-1+\left(\mathbf{x}-\boldsymbol{\mu}+\frac{1}{2} \mathbf{L}^{-1} \mathbf{m}\right)^{\mathrm{T}} \mathbf{L}\left(\mathbf{x}-\boldsymbol{\mu}+\frac{1}{2} \mathbf{L}^{-1} \mathbf{m}\right)+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{m}-\frac{1}{4} \mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m} .
$$

进行变量代换
$$
\mathbf{y}=\mathbf{x}-\boldsymbol{\mu}+\frac{1}{2} \mathbf{L}^{-1} \mathbf{m} .
$$

约束可重写为
$$
\int \exp \left\{\lambda-1+\mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{y}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{m}-\frac{1}{4} \mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m}\right\}\left(\mathbf{y}+\boldsymbol{\mu}-\frac{1}{2} \mathbf{L}^{-1} \mathbf{m}\right) \mathrm{d} \mathbf{y}=\boldsymbol{\mu} .
$$

和
$$
\int \exp \left\{\lambda-1+\mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{y}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{m}-\frac{1}{4} \mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m}\right\} \mathrm{d} \mathbf{y}=1 .
$$
因为$\mathbf y$的那一项对称，所以积分为0，因此
$$
-\frac{1}{2} \mathbf{L}^{-1} \mathbf{m}=\mathbf{0}
$$
最后得出
$$
p(\mathbf{x})=\exp \left\{\lambda-1+(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{L}(\mathbf{x}-\boldsymbol{\mu})\right\} .
$$

令$\mathbf{x}-\boldsymbol{\mu}=\mathbf{z}$
$$
\int \exp \left\{\lambda-1+\mathbf{z}^{\mathrm{T}} \mathbf{L z}\right\} \mathbf{z z}^{\mathrm{T}} \mathrm{d} \mathbf{x}=\mathbf{\Sigma} .
$$

且
$$
\int \exp \left\{\lambda-1+\mathbf{z}^{\mathrm{T}} \mathbf{L z}\right\} \text d\mathrm x=1
$$


解得$\mathbf{L}=-\frac{1}{2} \boldsymbol{\Sigma}$. ，并且
$$
\lambda-1=\ln \left\{\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}}\right\} .
$$
可知分布为高斯分布

- 回归问题

  给定损失函数
  $$
  L(y,f(\mathbf x,\alpha))=(y-f(\mathbf x,\alpha))^2\tag{1-3}
  $$
  则平均损失为
  $$
  \begin{aligned}
  R(\alpha)&=\int L(y,f(\mathbf x,\alpha))\mathrm{d}F(\mathbf x,y)\\
  &=\int (y-f(\mathbf x,\alpha))^2\mathrm{d}F(\mathbf x,y)\\
  &=\iint (y-f(\mathbf x,\alpha))^2\boldsymbol f(\mathbf x,y)\mathrm{d}\mathbf x\mathrm dy   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \boldsymbol f(\mathbf x,y)是联合概率密度
  \end{aligned}
  $$
  想要使得$R(\alpha)$最小，利用欧拉—拉格朗日方程计算$R(\alpha)$关于$f(\mathbf x,\alpha)$的导数，最后可以解得
  $$
  \begin{aligned}
  f(\mathbf x,\alpha)&=\int y\boldsymbol f(y|\mathbf x)\mathrm dy\\
  &=\int y \mathrm dF(y|\mathbf x)\\
  &=\Bbb E_y[y|\mathbf x]
  \end{aligned}
  $$
  这也透露回归问题的本质是逼近$y$的条件期望。
