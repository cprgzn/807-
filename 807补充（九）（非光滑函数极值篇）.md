# 807补充（九）（非光滑函数极值）

现在考虑非平滑凸目标函数的最小化 $\min _{\boldsymbol{x} \in \mathbb{R}^n} f(\boldsymbol{x})$, 其中 $f$ 为凸函数, 但是非平滑函数, 不可微分。
非平滑目标函数的常见例子如 $\|\boldsymbol{x}\|_1,\|\boldsymbol{x}\|_{\infty},\|\boldsymbol{A x}-\boldsymbol{b}\|_1$ 等。
由于非平滑函数 $f(\boldsymbol{x})$ 在 $\boldsymbol{x}$ 的梯度向量不存在, 所以基于梯度算法不适用。一个自然的问题是: 非平滑函数是否存在类似于梯度向量的某种 “广义梯度”?

## 一.次导数(subderivative)

> 定义：凸函数$f:\Bbb R→\Bbb R$在点$x_0$的次导数，是实数$g$使得： 
> $$
> f(x)-f\left(x_0\right) \geq g\left(x-x_0\right)
> $$



对上式变形再取极限得
$$
\begin{aligned}\lim _{x \rightarrow x_0^{-}} \frac{f(x)-f\left(x_0\right)}{x-x_0}\leqslant g,\lim _{x \rightarrow x_0^{+}} \frac{f(x)-f\left(x_0\right)}{x-x_0}\geqslant g\end{aligned}
$$


记
$$
\begin{aligned} & a=\lim _{x \rightarrow x_0^{-}} \frac{f(x)-f\left(x_0\right)}{x-x_0} \\ & b=\lim _{x \rightarrow x_0^{+}} \frac{f(x)-f\left(x_0\right)}{x-x_0}\end{aligned}
$$
可知$g\in[a,b]$,当$a=b$时导数存在，即次导数$g=f'(x_0)$

对于绝对值函数$y=|x|$,其次导数为
$$
\partial|x|= \begin{cases}\{-1\}, & x<0 \\ \{+1\}, & x>0 \\ {[-1,1],} & x=0\end{cases}
$$

## 二.次梯度（subgradient）

> 定义   设 $f$ 为凸函数, $\boldsymbol x$ 为定义域 $\operatorname{dom} f$ 中的一点.若向量 $\boldsymbol g \in \mathbb{R}^n$ 满足
> $$
> f(\boldsymbol y) \geqslant f(\boldsymbol x)+\boldsymbol g^{\mathrm{T}}(\boldsymbol y-\boldsymbol x), \quad \forall \boldsymbol y \in \operatorname{dom} f,
> $$
> 则称 $\boldsymbol g$ 为函数 $f$ 在点 $\boldsymbol x$ 处的一个次梯度. 

显然, 若 $f$ 是凸函数和可微分, 则 $f$ 在 $\boldsymbol{x}$ 的梯度 $\nabla f(\boldsymbol{x})$ 即是一个次梯度向量。因此, 梯度向量是次梯度向量的特例。一般说来, 一函数在某点 $\boldsymbol{x}$ 的次梯度可能有多个。

> 因此称集合
> $$
> \partial f(\boldsymbol x)=\left\{\boldsymbol g \mid \boldsymbol g \in \mathbb{R}^n, f(\boldsymbol y) \geqslant f(\boldsymbol x)+g^{\mathrm{T}}(\boldsymbol y-\boldsymbol x), \forall \boldsymbol y \in \operatorname{dom} f\right\}
> $$
> 为 $\boldsymbol f$ 在点 $\boldsymbol x$ 处的次微分.

函数 $f(\boldsymbol{x})$ 称为在点 $\boldsymbol{x}$ 是可次微分的 (subdifferentiable), 若它至少存在一个次梯度向量。函数 $f$ 在定义域上是可次微分的, 若它在所有点 $\boldsymbol{x} \in \operatorname{dom} f$ 是可次微分的。

> 定理  (次梯度存在性) :设 $f$ 为凸函数, $\operatorname{dom} f$ 为其定义域. 如果 $\boldsymbol x \in \operatorname{int} \operatorname{dom} f$, 则 $\partial f(\boldsymbol x)$ 是非空的, 其中 $\operatorname{int} \operatorname{dom} f$ 的含义是集合 $\operatorname{dom} f$ 的所有内点.

可以简单的认为凸函数一定存在次梯度。

>定理  设 $f(\boldsymbol x)$ 在 $\boldsymbol x_0 \in \operatorname{int} \operatorname{dom} f$ 处可微, 则
>$$
>\partial f\left(\boldsymbol x_0\right)=\left\{\nabla f\left(\boldsymbol x_0\right)\right\} .
>$$

证明. 因$f(\boldsymbol x)$是可微凸函数，则
$$
f(\boldsymbol y)\geqslant f(\boldsymbol x_0)+\nabla f(\boldsymbol x_0)^{\mathrm T}(\boldsymbol y-\boldsymbol x_0)
$$
可知梯度 $\nabla f\left(\boldsymbol x_0\right)$ 为次梯度.

下证 $f(\boldsymbol x)$ 在点 $x_0$ 处不可能有其他次梯度. 设 $\boldsymbol g \in \partial f\left(\boldsymbol x_0\right)$, 根据次梯度的定义, 对任意的非零 $\boldsymbol v \in \mathbb{R}^n$ 且 $\boldsymbol x_0+t \boldsymbol v \in \operatorname{dom} f, t>0$ 有
$$
f\left(\boldsymbol x_0+t \boldsymbol v\right) \geqslant f\left(\boldsymbol x_0\right)+t \boldsymbol g^{\mathrm{T}} \boldsymbol v .
$$

若 $\boldsymbol g \neq \nabla f\left(\boldsymbol x_0\right)$, 取 $\boldsymbol v=\boldsymbol g-\nabla f\left(\boldsymbol x_0\right) \neq 0$, 上式变形为
$$
\frac{f\left(\boldsymbol x_0+t \boldsymbol v\right)-f\left(\boldsymbol x_0\right)-t \nabla f\left(\boldsymbol x_0\right)^{\mathrm{T}} \boldsymbol v}{t\|\boldsymbol v\|} \geqslant \frac{\left(\boldsymbol g-\nabla f\left(\boldsymbol x_0\right)\right)^{\mathrm{T}} \boldsymbol v}{\|\boldsymbol v\|}=\|\boldsymbol v\| .
$$

不等式两边令 $t \rightarrow 0$, 根据 Fréchet 可微的定义, 左边趋于 0 , 而右边是非零正数, 可得到矛盾.

## 三.计算规则

我们不加证明地给出一些计算次梯度（次微分）的基本规则.

(1) 可微凸函数: 设 $f$ 为凸函数, 若 $f$ 在点 $x$ 处可微, 则 $\partial f(\boldsymbol x)=\{\nabla f(\boldsymbol x)\}$

(2) 凸函数的非负线性组合: 设 $f_1, f_2$ 为凸函数, 且满足
$$
\text { int dom } f_1 \cap \operatorname{dom} f_2 \neq \varnothing \text {, }
$$

而 $\boldsymbol x \in \operatorname{dom} f_1 \cap \operatorname{dom} f_2$. 若
$$
f(\boldsymbol x)=\alpha_1 f_1(\boldsymbol x)+\alpha_2 f_2(\boldsymbol x), \quad \alpha_1, \alpha_2 \geqslant 0,
$$

则 $f(\boldsymbol x)$ 的次微分
$$
\partial f(\boldsymbol x)=\alpha_1 \partial f_1(\boldsymbol x)+\alpha_2 \partial f_2(\boldsymbol x) .
$$
(3) 仿射变换: 设 $h$ 为凸函数, 并且函数 $f$ 满足
$$
f(\boldsymbol x)=h(\boldsymbol A \boldsymbol x+\boldsymbol b), \quad \forall \boldsymbol x \in \mathbb{R}^m,
$$
其中 $\boldsymbol A \in \mathbb{R}^{n \times m}, \boldsymbol b \in \mathbb{R}^n$. 若存在 $\boldsymbol x^{*} \in \mathbb{R}^m$, 使得 $\boldsymbol A \boldsymbol x^{*}+\boldsymbol b \in \operatorname{int}\operatorname{dom} h$,则
$$
\partial f(\boldsymbol x)=\boldsymbol A^{\mathrm{T}} \partial h(\boldsymbol A \boldsymbol x+\boldsymbol b), \quad \forall \boldsymbol x \in \operatorname{int} \operatorname{dom} f .
$$
(4) 凸函数之和的次微分 若 $f_1, \cdots, f_m$ 均为凸函数, 则函数 $f(\boldsymbol{x})=f_1(\boldsymbol{x})+\cdots+$ $f_m(\boldsymbol{x})$ 的次微分
$$
\partial f(\boldsymbol{x})=\partial f_1(\boldsymbol{x})+\cdots+\partial f_m(\boldsymbol{x})
$$
(5) 逐点极大函数的次微分 令 $f$ 是凸函数 $f_1, \cdots, f_m$ 的逐点极大函数, 即
$$
f(\boldsymbol{x})=\max _{i=1, \cdots, m} f_i(\boldsymbol{x})
$$

则
$$
\partial f(\boldsymbol{x})=\operatorname{conv}\left(\bigcup\left\{\partial f_i(\boldsymbol{x}) \mid f_i(\boldsymbol{x})=f(\boldsymbol{x})\right\}\right)
$$

即逐点极大函数 $f$ 的次微分是 “作用函数” (active function) $f_i(\boldsymbol{x})$ 在点 $\boldsymbol{x}$ 的次微分的并集的凸包。

![微信图片_20240115174452](E:\360MoveData\Users\Administrator\Desktop\Markdown\微信图片_20240115174452.png)

当$\boldsymbol x=\boldsymbol x_0$时，$f(\boldsymbol x_0)=f_1(\boldsymbol x_0)=f_2(\boldsymbol x_0)$,所以$\partial f(\boldsymbol x_0)=\text{conv}(\partial f_1(\boldsymbol x_0)\bigcup \partial f_2(\boldsymbol x_0))$,即$f_1(\boldsymbol x),f_2(\boldsymbol x)$在$\boldsymbol x_0$处的次微分形成的凸包（图上红色阴影部分）

$y=|x|=\text{max}(-x,x)$,利用性质（5）即可得出$y$在$0$点处的次微分为

$\text{conv}(-1\bigcup 1)=[-1,1]$

 $\left(\ell_1\right.$ 范数 $)$ 定义 $f: \mathbb{R}^n \rightarrow \mathbb{R}$ 为 $\ell_1$ 范数, 则对 $\boldsymbol x=\left(x_1, x_2, \cdots, x_n\right) \in$ $\mathbb{R}^n$, 有
$$
f(x)=\|\boldsymbol x\|_1=\max _{\boldsymbol s_i \in\{-1,1\}^n} \boldsymbol s^{\mathrm{T}} \boldsymbol x .
$$
于是
$$
\partial f(\boldsymbol x)=
\begin{bmatrix}
\partial f( x_1),\partial f( x_1),\cdots,\partial f( x_n)
\end{bmatrix}^\mathrm T
\\
\partial f( x_k)=\left\{\begin{array}{rr}
{[-1,1],} & x_k=0, \\
\{1\}, & x_k>0, \\
\{-1\}, & x_k<0 .
\end{array}\right.
$$
 ( $\ell_2$ 范数) 设 $f(\boldsymbol x)=\|\boldsymbol x\|_2$, 则 $f(\boldsymbol x)$ 在点 $\boldsymbol x=0$ 处不可微, 我们求其在该点处的次梯度. 由定义得
$$
\begin{aligned}
 \|\boldsymbol x\|_2 &\geqslant \boldsymbol g^{\top} \boldsymbol x \\
& \Updownarrow \\
 \|\boldsymbol x\|_2 &\geqslant\|\boldsymbol g\|_2\|\boldsymbol x\|_2 \cos \theta \\
&\Updownarrow\\
 1 &\geqslant\|\boldsymbol g\|_2 \cos \theta \\
 &\Updownarrow \\1&\geqslant\|\boldsymbol g\|_2 \\
&
\end{aligned}
$$

因此
$$
\partial f(0)=\left\{\boldsymbol g \mid\|\boldsymbol g\|_2 \leqslant 1\right\} .
$$

## 四.优化问题一阶条件

>定理  假设 $f$ 是凸函数, 则 $\boldsymbol x^*$ 为$f(\boldsymbol x)$ 的一个全局极小点当且仅当
>$$
>0 \in \partial f\left(\boldsymbol x^*\right) .
>$$
>

证明. 因为 $\boldsymbol x^*$ 为全局极小点, 所以
$$
f(\boldsymbol y) \geqslant f\left(\boldsymbol x^*\right)=f\left(\boldsymbol x^*\right)+\boldsymbol 0^{\mathrm{T}}\left(\boldsymbol y-\boldsymbol x^*\right), \quad \forall \boldsymbol y \in \mathbb{R}^n .
$$

因此, $\boldsymbol 0 \in \partial f\left(\boldsymbol x^*\right)$.
再证充分性. 如果 $\boldsymbol 0 \in \partial f\left(\boldsymbol x^*\right)$, 那么根据次梯度的定义
$$
f(\boldsymbol y) \geqslant f\left(\boldsymbol x^*\right)+\boldsymbol 0^{\mathrm{T}}\left(\boldsymbol y-\boldsymbol x^*\right)=f\left(\boldsymbol x^*\right), \quad \forall \boldsymbol y \in \mathbb{R}^n .
$$

因而 $\boldsymbol x^*$ 为一个全局极小点.(**注意并未保证这样的解唯一，最小值唯一但最优解不一定唯一，例如$y=\text{max}(x,0)$,仅当$f$是强凸函数时解才唯一**)

在实际问题中, 目标函数不一定是凸函数, 但它可以写成一个光滑函数与一个非光滑凸函数的和, 比如 LASSO 问题; 也可能是非凸的, 例如神经网络的损失函数. 因此研究此类问题的最优性条件十分必要. 这里, 我们考虑一般复合优化问题
$$
\min _{\boldsymbol x \in \mathbb{R}^n} \psi(\boldsymbol x) \xlongequal{\text { def }} f(\boldsymbol x)+h(\boldsymbol x),
$$

其中 $f$ 为光滑函数 (可能非凸),$h$ 为凸函数 (可能非光滑). 对于其任何局部最优解, 我们给出如下一阶必要条件:

>定理 (复合优化问题一阶必要条件) 令 $\boldsymbol x^*$ 为问题 $\min _{\boldsymbol x \in \mathbb{R}^n} \psi(\boldsymbol x) \xlongequal{\text { def }} f(\boldsymbol x)+h(\boldsymbol x)$ 的一个局部极小点, 那么
>$$
>-\nabla f\left(\boldsymbol x^*\right) \in \partial h\left(\boldsymbol x^*\right),
>$$
>
>其中 $\partial h\left(\boldsymbol x^*\right)$ 为凸函数 $h$ 在点 $\boldsymbol x^*$ 处的次梯度集合.

## 五.例

-  $\ell_1$ 范数优化问题

  其形式可以写成

$$
\min _{\boldsymbol x \in \mathbb{R}^n} \psi(\boldsymbol x) \xlongequal{\text { def }} f(\boldsymbol x)+\mu\|\boldsymbol x\|_1,
$$

​		其中 $f(\boldsymbol x): \mathbb{R}^n \rightarrow \mathbb{R}$ 为光滑函数, 正则系数 $\mu>0$ 用来调节解的稀疏度，尽管 $\|\boldsymbol x\|_1$ 不是可微的, 但我们可以计算其次微分
$$
\partial_i\|x\|_1= \begin{cases}\{1\}, & x_i>0, \\ {[-1,1],} & x_i=0, \\ \{-1\}, & x_i<0 .\end{cases}
$$

​		因此, 如果 $x^*$ 一个局部最优解, 那么其满足
$$
-\nabla f\left(\boldsymbol x^*\right) \in \mu \partial\left\|\boldsymbol x^*\right\|_1,
$$
​		即
$$
\nabla_i f\left(x^*\right)= \begin{cases}-\mu, & x_i^*>0, \\ a \in[-\mu, \mu], & x_i^*=0, \\ \mu, & x_i^*<0 .\end{cases}
$$

​		进一步地, 如果 $f(x)$ 是凸的（比如在 LASSO 问题中 $f(x)=\frac{1}{2} \| A x- b \|^2$ ), 那么满足上式的 $x^*$ 就是全局最优解.

- 平均绝对误差（mae）

  其中$\boldsymbol x\in \Bbb R^n$,设 $\left\{x_{(1)}^j, \cdots x_{(n)}^j\right\}=\left\{x_1^j, \cdots x_n^j\right\}$,其形式可以写为
  $$
  \min _{\boldsymbol x \in \mathbb{R}^n} y(\boldsymbol x) \xlongequal{\text { def }} \sum_i^N|\boldsymbol x-\boldsymbol x_{(i)}|=\sum_i^N\sum_j^n|x^j-x_{(i)}^j|=\sum_j^n\sum_i^N|x^j-x_{(i)}^j|,
  $$
  由于彼此维度无关，则优化问题可以写为关于每一维标量优化，计算其次微分得
  $$
  \begin{aligned}
  \partial y(x^j)&=
  \begin{cases}
  1 ,&x^j\geqslant x_{(1)}^j\\
  [-1,1],&x^j= x_{(1)}^j\\
  -1,&x^j\leqslant x_{(1)}^j
  \end{cases}+\cdots+\begin{cases}
  1 ,&x^j\geqslant x_{(N)}^j\\
  [-1,1],&x^j= x_{(N)}^j\\
  -1,&x^j\leqslant x_{(N)}^j\\
  \end{cases}\\
  \end{aligned}
  $$
  又因
  $$
  0 \in \partial y\left( x^j\right) .
  $$
  

  **所以当N为奇数时，有唯一解即为中位数，当N为偶数时，$x^j\in[x^j_{[\frac{N}{2}]},x^j_{[\frac{N}{2}]+1}]$都是其最优解。**

​		**即mae偏向于回归中位数**