# 807补充（二）

**注：在本文中不考虑复数矩阵的可能性，仅考虑实矩阵**

## 符号约定

| $\mathbf{X},\mathbf{A},\mathbf{B}$ | 矩阵 |          $\mathbf{F(\cdot)}$          | 输出为矩阵的函数 |
| :--------------------------------: | :--: | :-----------------------------------: | :--------------: |
| $\mathbf{x},\mathbf{y},\mathbf{z}$ | 向量 | $\mathbf{f(\cdot)},\mathbf{g(\cdot)}$ | 输出为向量的函数 |
|               $x,y$                | 标量 |             $f ( \cdot)$              | 输出为标量的函数 |

在上篇中我们使用定义计算了矩阵函数的梯度，并且给出了多元函数二阶导海瑟矩阵的表达式，但是在实际在计算中还是略微繁琐，在本文中将给出计算梯度标准型。

## 一.矩阵的Hadamard积



$m \times n$矩阵$\mathbf A=[a_{ij}]$与$m \times n$矩阵$\mathbf B=[b_{ij}]$的Hadamard积记作$\mathbf A\odot\mathbf B$，它仍然是一个$m\times n$矩阵，其元素定义为两个矩阵对应元素的乘积。
$$
(\mathbf A\odot\mathbf B)_{ij}=a_{ij}b_{ij}
$$
即Hadamard积是一映射$\Bbb R^{m\times n}\times \Bbb R^{m\times n}\rightarrow\Bbb R^{m\times n}$

Hadamard积的一些性质如下。
$$
\begin{aligned}
&(1)(\mathbf A\odot \mathbf B)^T=\mathbf A^T\odot\mathbf B^T\\
&(2)c(\mathbf A\odot\mathbf B)=(c\mathbf A)\odot\mathbf B=\mathbf A\odot (c\mathbf B)\\
&(3)\mathbf A\odot \mathbf B=\mathbf B\odot \mathbf A\\
&(4)若m\times m矩阵\mathbf A,\mathbf B是正定(或者半正定)的，则它们的Hadamard积也是正定(或半正定)的。\\
&(5)\mathrm{tr}(\mathbf A^T(\mathbf B\odot\mathbf C))=\mathrm{tr}((\mathbf A^T\odot\mathbf B^T)\mathbf C)\\
&(6)\mathrm{vec}(\mathbf A\odot\mathbf X)=\mathrm{diag}(\mathbf A)\mathrm{vec}(\mathbf X)
\end{aligned}
$$

## 二.矩阵的Kronecker积

两个矩阵的Kronecker积分为左Kronecker积和右Kronecker积。

$m \times n$矩阵$\mathbf A=[a_{ij}]$与$p \times q$矩阵$\mathbf B=[b_{ij}]$的右Kronecker积记作$\mathbf A\otimes\mathbf B$，它是一个$mp\times nq$矩阵，定义为
$$
\mathbf A\otimes\mathbf B=[a_{ij}\mathbf B]_{i=1,j=1}^{m,n}=\begin{bmatrix}a_{11}\mathbf B &a_{12}\mathbf B&\cdots&a_{1n}\mathbf B\\ 
a_{21}\mathbf B &a_{22}\mathbf B&\cdots&a_{2n}\mathbf B\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}\mathbf B &a_{m2}\mathbf B&\cdots&a_{mn}\mathbf B
\end{bmatrix}
$$


$m \times n$矩阵$\mathbf A=[a_{ij}]$与$p \times q$矩阵$\mathbf B=[b_{ij}]$的左Kronecker积记作$[\mathbf A\otimes\mathbf B]_{\mathrm{left}}$，它是一个$mp\times nq$矩阵，定义为
$$
[\mathbf A\otimes\mathbf B]_{\mathrm{left}}=[b_{ij}\mathbf A]_{i=1,j=1}^{m,n}=\begin{bmatrix}b_{11}\mathbf A &b_{12}\mathbf A&\cdots&b_{1n}\mathbf A\\ 
b_{21}\mathbf A &b_{22}\mathbf A&\cdots&b_{2n}\mathbf A\\
\vdots&\vdots&\ddots&\vdots\\
b_{m1}\mathbf A &b_{m2}\mathbf A&\cdots&b_{mn}\mathbf A
\end{bmatrix}
$$
显然，矩阵的左Kronecker积可以用右Kronecker积表示。默认都采用右Kronecker积表示Kronecker积。

Kronecker积有以下常用性质
$$
\begin{aligned}
&(1)\mathbf A\otimes\mathbf B \neq \mathbf B\otimes\mathbf A\\
&(2)\alpha\mathbf A\otimes\mathbf \beta B=\alpha\beta(\mathbf A\otimes\mathbf B)\\
&(3)(\mathbf A\mathbf B)\otimes(\mathbf C\mathbf D)=(\mathbf A\otimes\mathbf C)(\mathbf B\otimes\mathbf D)\\
&(4)(\mathbf A\otimes\mathbf B)^T=\mathbf A^T\otimes\mathbf B^T\\
&(5)(\mathbf A\otimes\mathbf B)^{-1}=\mathbf A^{-1}\otimes\mathbf B^{-1}\\
&(6)\mathbf{vec}(\mathbf A\mathbf X \mathbf B)=(\mathbf B^T\otimes \mathbf A)\mathrm{vec}(\mathbf X)\\
&(7)\mathrm{vec}(\mathbf a \mathbf b^T)=\mathbf b\otimes \mathbf a
\end{aligned}
$$

## 三.逐元素函数

假设一个函数$f(x)$的输出是标量$x$,对于一组$K$个标量$x_1,x_2,\cdots,x_k$我们可以通过$f(x)$得到另外一组$K$个标量$z_1,z_2,z_3,\cdots,z_k$。
$$
z_k=f(x_k),\ \ \ \ \ \ \ \ \forall k=1,\cdots,K
$$
我们定义$\mathbf x=[x_1,\cdots,x_k]^T,\mathbf z=[z_1,\cdots,z_k]^T$。
$$
\mathbf z=\mathbf{f(x)}
$$
其中$f(x)$是按位运算，即$[\mathbf f(\mathbf x)]_k=f(x_k)$。这样的函数就是逐元素函数，将向量变元推广至矩阵变元也成立。

当$x$为标量时，$f(x)$的导数记为$f'(x)$，当输入为K维向量$\mathbf x = [x_1,\cdots,x_k]^T$时，其导数为一个对角阵。
$$
\begin{aligned}
\frac{\partial \mathbf{f(x)}}{\partial \mathbf x}&=
\left[
\begin{matrix}
f'(x_1) & 0 & \cdots & 0\\
0 & f'(x_2) & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & f'(x_k)
\end{matrix}
\right]\\
\\
&=\mathrm{diag}(\mathbf f'(\mathbf x))
\end{aligned}
$$


## 四.实值函数的微分标准型

矩阵微分用符号$\mathrm{d}\mathbf X$表示，定义为$\mathrm{d}\mathbf X\overset{\mathrm{def}}{=}[\mathrm{d}x_{ij}]^{m,n}_{i=1,j=1}$

下面是矩阵微分常用的性质。
$$
\begin{aligned}
&(1)\mathrm{d}(\mathrm{tr}(\mathbf X))=\mathrm{tr}(\mathrm{d}(\mathbf X))\\
&(2)\mathrm{d}(\mathbf X \mathbf Y)=(\mathrm d\mathbf X)\mathbf Y+\mathbf X(\mathrm d\mathbf Y)\\
&(3)\mathrm d(\mathbf X)^T=(\mathrm d\mathbf X)^T\\
&(4)\mathrm d(\alpha\mathbf X+\beta\mathbf Y)=\alpha\mathrm d\mathbf X+\beta\mathrm d\mathbf Y\\
&(5)常数矩阵的微分为0 \ \ \ \ \ \ \ \ \mathrm d(\mathbf A)=0\\
&(6)\mathrm{d \mathbf{X^{-1}}}=-\mathbf{X^{-1}}\mathrm{d \mathbf{X}}\mathbf{X^{-1}}\ \ \ \ \ 可对\mathbf I两边求微分得出\\
&(7)\mathrm{d |X|}=|\mathbf{X}|\mathrm{tr(\mathbf X^{-1} \mathrm{d \mathbf{X}})}\ \ \ \ \ 可用Laplace展开证明\\
&(8)\mathrm d(\mathbf{X}\odot \mathbf{Y})=(\mathrm d\mathbf{X})\odot \mathbf{Y}+\mathbf{X}\odot(\mathrm d\mathbf{Y})\\
&(9)\mathrm d(\mathbf{X}\otimes \mathbf{Y})=(\mathrm d\mathbf{X})\otimes \mathbf{Y}+\mathbf{X}\otimes(\mathrm d\mathbf{Y})\\
&(10)\mathrm d(\mathrm{vec(\mathbf{X})})=\mathrm{vec}(\mathrm d\mathbf{X})\\
&(11)\mathrm d\log\mathbf{X}=\mathbf{X}^{-1}\mathrm d\mathbf{X}\\
&(12)\mathrm d \mathbf f(\mathbf X)=\mathbf f'(\mathbf X)\odot\mathrm d\mathbf X     (\mathbf f是逐元素函数)
\end{aligned}
$$
考虑实值标量函数$f(\mathbf x)$,其全微分形式为
$$
\begin{aligned}
\mathrm df&=\frac{\partial f}{\partial x_1}\mathrm d x_1+\frac{\partial f}{\partial x_2}\mathrm d x_2+\cdots+\frac{\partial f}{\partial x_n}\mathrm d x_n\\
&=[\frac{\partial f}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n}]\begin{bmatrix}\mathrm dx_1\\ \vdots \\\mathrm dx_n
\end{bmatrix}\\
&=(\frac{\partial f}{\partial \mathbf x})^T\mathrm d\mathbf x
\end{aligned}
$$
进一步考察标量函数$f(\mathbf X)$，其变元为$m\times n$矩阵$\mathbf X$,由全微分形式易证
$$
\begin{aligned}
\mathrm df&=\frac{\partial f}{\partial x_{11}}\mathrm d  x_{11}+\frac{\partial f}{\partial  x_{12}}\mathrm d  x_{12}+\cdots+\frac{\partial f}{\partial  x_{mn}}\mathrm d  x_{mn}\\
&=[\frac{\partial f}{\partial  x_{11}},\cdots,\frac{\partial f}{\partial  x_{mn}}]\begin{bmatrix}\mathrm d x_{11}\\ \vdots \\\mathrm d x_{mn}
\end{bmatrix}\\
&=(\frac{\partial f}{\partial \mathrm{vec}(\mathbf X)})^T\mathrm d\mathrm{vec}(\mathbf X)\\
&=(\mathrm{vec}( \frac{\partial f(\mathbf X)}{\partial \mathbf X}))^T\mathrm d(\mathrm{vec}\mathbf X)\ \ \ \ 与矩阵内积的形式一致\\
&=\mathrm{tr}((\frac{\partial f(\mathbf X)}{\partial \mathbf X})^T\mathrm d\mathbf X)
\end{aligned}
$$
由此得出以下重要结论
$$
\mathrm d f(\mathbf x)=(\frac{\partial f(\mathbf x)}{\partial \mathbf x})^T\mathrm d\mathbf x\\
\mathrm df(\mathbf X)=\mathrm{tr}((\frac{\partial f(\mathbf X)}{\partial \mathbf X})^T\mathrm d\mathbf X)
$$
即只要找到$\mathrm d f(\mathbf x)$和$\mathrm d\mathbf x$之间的关系即可得出函数的梯度

例

1. $y=\mathbf a^T\mathbf{Xb}$
   $$
   \begin{aligned}
   \mathrm dy&=\mathrm d(\mathbf a^T)\mathbf{Xb}+\mathbf a^T\mathrm d(\mathbf{Xb})\\
   &=\mathbf a^T\mathrm {(d\mathbf{X})}\mathbf b\\
   &=\mathrm{tr}(\mathbf a^T\mathrm {(d\mathbf{X})}\mathbf b)\\
   &=\mathrm{tr}(\mathbf b\mathbf a^T\mathrm {(d\mathbf{X}))}\\
   \end{aligned}
   $$
   所以$\frac{\partial y}{\partial \mathbf X}=(\mathbf{b}\mathbf{a}^T)^T=\mathbf a\mathbf b^T$

2. 方差的最大似然估计,随机变量$\mathbf x$服从$\mathcal N(\mathbf \mu,\mathbf \Sigma)$,现有样本$\mathbf x_1,\mathbf x_2,\mathbf x_3,\cdots,\mathbf x_n$,求协方差矩阵的最大似然估计。

   对数似然函数为
   $$
   L=\log |\mathbf \Sigma|+\frac{1}{N}\sum_i^N(\mathbf x_i-\mathbf \mu)^T\mathbf\Sigma^{-1}(\mathbf x_i-\mathbf \mu)
   $$
   
   $$
   \begin{aligned}
   \mathrm{d}L&=\frac{1}{|\mathbf \Sigma|}\mathrm d|\mathbf \Sigma|+\frac{1}{N}\sum_i^N(\mathbf x_i-\mathbf \mu)^T(\mathrm d\mathbf\Sigma^{-1})(\mathbf x_i-\mathbf \mu)\\
   &=\mathrm{tr}(\mathbf \Sigma^{-1}\mathrm d\mathbf \Sigma)-\frac{1}{N}\sum_i^N(\mathbf x_i-\mathbf \mu)^T(\mathbf{\Sigma^{-1}}\mathrm{d \mathbf{\Sigma}}\mathbf{\Sigma^{-1}})(\mathbf x_i-\mathbf \mu)\\
   &=\mathrm{tr}(\mathbf \Sigma^{-1}\mathrm d\mathbf \Sigma)-\frac{1}{N}\mathrm{tr}(\sum_i^N((\mathbf x_i-\mathbf \mu)(\mathbf x_i-\mathbf \mu)^T\mathbf{\Sigma^{-2}}\mathrm{d \mathbf{\Sigma}})\\
   &=\mathrm{tr}((\mathbf \Sigma^{-1}\mathrm -\frac{1}{N}\sum_i^N((\mathbf x_i-\mathbf \mu)(\mathbf x_i-\mathbf \mu)^T\mathbf{\Sigma^{-2}})\mathrm{d \mathbf{\Sigma}})
   \end{aligned}
   $$
   所以要使$\nabla L=0$应使
   $$
   \mathbf \Sigma=\frac{1}{N}\sum_i^N((\mathbf x_i-\mathbf \mu)(\mathbf x_i-\mathbf \mu)^T
   $$
   
3. 最二乘估计。$L=||\mathbf X\mathbf w-\mathbf y||^2$
   $$
   \begin{aligned}
   \mathrm dL&=\mathrm d[(\mathbf X\mathbf w-\mathbf y)^T(\mathbf X\mathbf w-\mathbf y)]\\
   &=2(\mathbf X\mathbf w-\mathbf y)^T\mathrm d\mathbf w
   \end{aligned}
   $$
   

   所以$\nabla L=2\mathbf X(\mathbf X\mathbf w-\mathbf y)$,为使梯度为零，即可解出



## 五.复杂函数的导数

假设$\mathbf Y$是关于$\mathbf X$的函数，若已经求出目标函数$f$关于$\mathbf Y$的梯度，依靠微分标准型即可得出目标函数$f$关于$\mathbf X$的梯度。

例

1. $f=f(\mathbf Y)$,$\mathbf Y=\mathbf B\mathbf X$
   $$
   \begin{aligned}
   \mathrm df&=\mathrm{tr}((\frac{\partial f}{\partial \mathbf Y})^T\mathrm d\mathbf Y)\\
   &=\mathrm{tr}((\frac{\partial f}{\partial \mathbf Y})^T\mathrm d(\mathbf {BX}))\\
   &=\mathrm{tr}((\frac{\partial f}{\partial \mathbf Y})^T\mathbf B\mathrm d\mathbf {X})\\
   &=\mathrm{tr}((\frac{\partial f}{\partial \mathbf X})^T\mathrm d\mathbf X)
   \end{aligned}
   $$
   即可得出$\frac{\partial f}{\partial \mathbf X}=\mathbf B^T\frac{\partial f}{\partial \mathbf Y}$

   ​	

易证以下常用结论

1. $若\mathbf{x} \in R^{m},\mathbf{y=f(x)} \in R^{n},\mathbf{z = g(x)}\in R^{n}则$
   $$
   \frac{\partial \mathbf{y}^T\mathbf{z}}{\partial \mathbf x}=\frac{\partial \mathbf{y}}{\partial \mathbf x}\mathbf{z}+\frac{\partial \mathbf{z}}{\partial \mathbf x}\mathbf{y}  \ \ \ \ \in R^m \tag{3-3}
   $$

2. $若\mathbf{x} \in R^{m},y=f(\mathbf x) \in R,\mathbf{z = g(\mathbf x)}\in R^{n}则$
   $$
   \frac{\partial y\mathbf{z}}{\partial \mathbf x}=\frac{\partial y}{\partial \mathbf x}\mathbf{z}^T+\frac{\partial \mathbf{z}}{\partial \mathbf x}y \ \ \ \ \in R^{m \times n} \tag{3-5}
   $$

3. $若\mathbf{x} \in R^{m},\mathbf{y=f(x)} \in R^{n},\mathbf{z = g(y)}\in R^{k}则$

   
   $$
   \frac{\partial \mathbf{z}}{\partial \mathbf{x}}=\frac{\partial \mathbf{y}}{\partial\mathbf{x}}\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\ \ \ \in R^{m \times k}\tag{3-2}
   $$

4. 若$f(\mathbf X),g(\mathbf X)$都是矩阵$\mathbf X$的实值函数，则
   $$
   \frac{\partial [f(\mathbf X)g(\mathbf X)]}{\partial \mathbf X}=f(\mathbf X)\frac{\partial g(\mathbf X)}{\partial \mathbf X}+g(\mathbf X)\frac{\partial f(\mathbf X)}{\partial \mathbf X}
   $$
   
5. 若$g(\mathbf X)\neq 0$
   $$
   \frac{\partial [f(\mathbf X)/g(\mathbf X)]}{\partial \mathbf X}=\frac{1}{g(\mathbf X)^2}[g(\mathbf X)\frac{\partial f(\mathbf X)}{\partial \mathbf X}-f(\mathbf X)\frac{\partial g(\mathbf X)}{\partial \mathbf X}]
   $$
   
6. 
   $$
   \frac{\partial \mathbf{z}}{\partial \mathbf{x}}=\frac{\partial \mathbf{y}}{\partial\mathbf{x}}\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\ \ \ \in R^{m \times k}\tag{3-2}
   $$
   **注意顺序是否颠倒（如果顺序不对则维度不匹配无法相乘）。**

## 六.激活函数的导数

1. $\mathbf y=\frac{1}{1+\exp(-\mathbf x)}$

   标准的标量Sigmod函数的导数为。
   $$
   f'(x)=f(x)(1-f(x))
   $$
   当输入为$K$维向量时，其导数为
   $$
   \frac{\partial\mathbf f(\mathbf x)}{\partial \mathbf x}=\mathrm{diag}(f(\mathbf x)\odot(1-f(\mathbf x)))
   $$

2. Softmax函数的导数。

   **Softmax**函数定义为
   $$
   z_k=\mathrm{sfotmax(x_k)}=\frac{\exp(x_k)}{\sum_{i=1}^{K}\exp(x_i)}
   $$
   用$K$维向量$\mathbf x=[x_1,\cdots,x_k]^T$来表示Softmax函数的输入，
   $$
   \begin{aligned}
   \mathbf z&=\mathrm{softmax}(\mathbf x)\\
   &=\frac{1}{\sum_{i=1}^{K}\exp(x_k)}\left[
   \begin{matrix}
   \exp(x_1) \\
   \exp(x_2) \\
   \vdots\\
   \exp(x_k)
   \end{matrix}
   \right]\\
   &=\frac{\exp(\mathbf x)}{\mathbf 1_K^T\exp(\mathbf x)}
   \end{aligned}
   $$
   Softmax函数的导数为
   $$
   \begin{aligned}
   \frac{\partial \mathrm{softmax(\mathbf x)}}{\partial \mathbf x}&=\frac{\partial \frac{\exp(\mathbf x)}{ \mathbf 1_K^T\exp(\mathbf x)}}{\partial \mathbf x}\\
   \\
   &=\frac{1}{\mathbf 1_k^T\exp(\mathbf x)}\frac{\partial \exp(\mathbf x)}{\partial \mathbf x}+\frac{\partial \frac{1}{\mathbf 1_k^T\exp(\mathbf x)}}{\partial \mathbf x}(\exp(\mathbf x))^T\\
   \\
   &=\frac{\mathrm{diag}(\exp(\mathbf x))}{\mathbf 1_k^T\exp(\mathbf x)}-(\frac{1}{(\mathbf 1_K^T\exp(\mathbf x))^2})\frac{(\partial \mathbf 1_K^T\exp(\mathbf x))}{\partial \mathbf x}(\exp(\mathbf x))^T\\
   \\
   &=\frac{\mathrm{diag}(\exp(\mathbf x))}{\mathbf 1_k^T\exp(\mathbf x)}-(\frac{1}{(\mathbf 1_K^T\exp(\mathbf x))^2})\mathrm{diag(\exp(\mathbf x))}\mathbf 1_K^T(\exp(\mathbf x))^T\\
   \\
   &=\frac{\mathrm{diag}(\exp(\mathbf x))}{\mathbf 1_k^T\exp(\mathbf x)}-(\frac{1}{(\mathbf 1_K^T\exp(\mathbf x))^2})\exp(\mathbf x)(\exp(\mathbf x))^T\\
   \\
   &=\mathrm{diag(\frac{\exp(\mathbf x)}{\mathbf 1_K^T\exp\mathbf x})}-\frac{\exp(\mathbf x)}{\mathbf 1_K^T\exp(\mathbf x)}\frac{(\exp(\mathbf x))^T}{\mathbf 1_K^T\exp(\mathbf x)}\\
   \\
   &=\mathrm{diag(softmax(\mathbf x))}-\mathrm{softmax(\mathbf x)}\mathrm{softmax(\mathbf x)}^T
   \end{aligned}
   $$
   

## 七.多层感知机反向传播

已知$\mathbf a^d,\mathbf z^d,\mathbf W^d,\mathbf b^d,\mathbf f$，分别为第d层已激活输出，未激活输出，权重矩阵,偏置,激活函数。
$$
\mathbf a^{d}=\mathbf f(\mathbf z^d)\\
\mathbf z^d=\mathbf W^d\mathbf a^{d-1}+\mathbf b^d
$$
现求损失函数对于第d层权重向量偏导。
$$
\begin{aligned}
\mathrm dL&=\mathrm{tr}((\frac{\partial L}{\partial \mathbf W^d})^T\mathrm d\mathbf W^d)\\
&=\mathrm{tr}((\frac{\partial L}{\partial \mathbf z^d})^T\mathrm d\mathbf z^d)\\
&=\mathrm{tr}((\frac{\partial L}{\partial \mathbf z^d})^T\mathrm d(\mathbf {W^d a^{d-1}}))\\
&=\mathrm{tr}(\mathbf a^{d-1}(\frac{\partial L}{\partial \mathbf z^d})^T\mathrm d\mathbf {W^d})
\end{aligned}
$$


所以$\frac{\partial L}{\partial \mathbf W^d}=\frac{\partial L}{\partial \mathbf z^d}(\mathbf a^{d-1})^T$

定义$\frac{\partial L}{\partial \mathbf z^d}\overset{\mathrm{def}}{=}\mathbf \delta^{d}$,记作每层的误差。
$$
\begin{aligned}
\mathbf \delta^{d}&=\frac{\partial L}{\partial \mathbf z^d}\\
&=\frac{\partial \mathbf a^d}{\partial \mathbf z^d}\frac{\partial \mathbf z^{d+1}}{\partial \mathbf a^d}\frac{\partial L}{\partial \mathbf z^{d+1}}\\
\end{aligned}
$$
又因
$$
\frac{\partial \mathbf a^d}{\partial\mathbf z^{d}}=\mathbf{diag}(\mathbf f'(\mathbf z^d))\\
\frac{\partial \mathbf z^{d+1}}{\partial\mathbf a^{d}}=(\mathbf W^{d+1})^T
$$
所以最后的结果为
$$
\mathbf \delta^d=\mathbf{diag}(\mathbf f'(\mathbf z^d))(\mathbf W^{d+1})^T\mathbf \delta^{d+1}
\\
\frac{\partial L}{\partial \mathbf W^d}=\mathbf \delta^d(\mathbf a^{d-1})^T1
$$
